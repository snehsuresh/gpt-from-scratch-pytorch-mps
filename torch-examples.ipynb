{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00126505\n",
      "CPU times: user 655 μs, sys: 835 μs, total: 1.49 ms\n",
      "Wall time: 1.91 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start_time = time.time()\n",
    "# matrix operations here\n",
    "zeros = torch.zeros(1, 1)\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"{elapsed_time:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the time library to record execution time\n",
    "%%time to record the time taken for the cell to execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS time => 0.04584384\n",
      "CPU time => 0.09187579\n"
     ]
    }
   ],
   "source": [
    "torch_rand1 = torch.rand(100, 100, 100, 100).to(device)\n",
    "torch_rand2 = torch.rand(100, 100, 100, 100).to(device)\n",
    "np_rand1 = torch.rand(100, 100, 100, 100)\n",
    "np_rand2 = torch.rand(100, 100, 100, 100)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rand = (torch_rand1 @ torch_rand2)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"MPS time => {elapsed_time:.8f}\")\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rand = np.multiply(np_rand1, np_rand2)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"CPU time => {elapsed_time:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# embeddings, torch.stack, torch.multinomial, torch.tril, torch.triu, input.T / input.transpose, nn.Linear, torch.cat, F.softmax (show all the examples of functions/methods with pytorch docs)\n",
    "\n",
    "\n",
    "# Define a probability tensor\n",
    "probabilities = torch.tensor([0.1, 0.9])\n",
    "# 10% or 0.1 => 0, 90% or 0.9 => 1. each probability points to the index of the probability in the tensor\n",
    "# Draw 5 samples from the multinomial distribution\n",
    "samples = torch.multinomial(probabilities, num_samples=10, replacement=True)\n",
    "print(samples)\n",
    "# Each value in this tensor represents the probability of selecting a corresponding index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([1, 2, 3, 4])\n",
    "out = torch.cat((tensor, torch.tensor([5])), dim=0)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = torch.tril(torch.ones(5, 5))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [0., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = torch.triu(torch.ones(5, 5))\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Masking in Sequence Models\n",
    "\n",
    "* And then we have a mask fill. \n",
    "* This one's going to be very important later because in order to actually get to this point, all we do is we just exponentiate every element in here. \n",
    "* So if you exponentiate zero, if you exponentiate zero, it'll become one. If you exponentiate negative infinity, it'll become zero. \n",
    "* All that's going on here is we're doing approximately 2.71. And this is a constant that we use in the dot exp function. \n",
    "* And then we're putting this to whatever power is in that current slot. So we have a zero here. So 2.71 to the zeroth is equal to one, 2.71 to the one is equal to 2.71. \n",
    "* And then, 2.71 to the negative infinity is, of course, zero. So that's pretty much how we get from this to this. And we're simply just masking these over. \n",
    "* So that's great. And I sort of showcase what the exp does. We're just using this one right here. We're using this output and we're just plugging it into here. \n",
    "* So it'll go from negative infinity to zero and then zero to one. So that's how we get from here to here.\n",
    "\n",
    "## Why Did We Make the TRIL Values -inf?\n",
    "\n",
    "We set the above-diagonal values to `-inf` instead of simply making them zero because of how the **softmax** function operates in neural networks, particularly in autoregressive models.\n",
    "\n",
    "1. **Masking Future Information**:\n",
    "   In autoregressive models, we need to predict the next element in a sequence using only the elements that have already been seen. The above-diagonal elements represent future information that should not be accessible during prediction. Masking these elements ensures that the model does not \"peek\" at future values.\n",
    "\n",
    "2. **Effect of Exponentiation**:\n",
    "   When we exponentiate the tensor values during calculations, setting above-diagonal elements to `-inf` results in:\n",
    "   - \\( e^{-\\infty} = 0 \\)\n",
    "   - This means these positions contribute zero to the softmax computation, effectively removing their influence.\n",
    "\n",
    "3. **Using Zeroes**:\n",
    "   If we set the above-diagonal elements directly to zero:\n",
    "   - \\( e^0 = 1 \\)\n",
    "   - These values would still contribute to the sum in the softmax denominator, allowing them to influence the output probabilities, which we do not want.\n",
    "\n",
    "### Example Matrix\n",
    "Consider this example matrix:\n",
    "```python\n",
    "tensor([[1., 2., 3.],\n",
    "        [4., 5., 6.],\n",
    "        [7., 8., 9.]])\n",
    "```\n",
    " - Here, the values above the diagonal (2, 3, 6) represent future information that the model should not use for predictions.\n",
    " - By masking them with -inf, we ensure that when the softmax function is applied, those positions will contribute zero probabilities.\n",
    "### Conclusion\n",
    "The use of -inf for the upper triangular elements is a crucial step in maintaining the causal relationship in sequence predictions. It ensures that the model learns to make predictions based solely on the available past and present information, thereby preventing it from using any future data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = torch.zeros(5, 5).masked_fill(torch.tril(torch.ones(5, 5)) == 0, float('-inf'))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7183, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [2.7183, 2.7183, 1.0000, 1.0000, 1.0000],\n",
       "        [2.7183, 2.7183, 2.7183, 1.0000, 1.0000],\n",
       "        [2.7183, 2.7183, 2.7183, 2.7183, 1.0000],\n",
       "        [2.7183, 2.7183, 2.7183, 2.7183, 2.7183]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 4])\n",
      "torch.Size([2, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "input = torch.zeros(2, 3, 4)\n",
    "out1 = input.transpose(0, 1)\n",
    "out2 = input.transpose(-2,-1)\n",
    "print(out1.shape)\n",
    "print(out2.shape)\n",
    "# torch.permute works the same but you provide the new order of dimensions instead of the dimensions you'd like to swap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.tensor([1, 2, 3])\n",
    "tensor2 = torch.tensor([4, 5, 6])\n",
    "tensor3 = torch.tensor([7, 8, 9])\n",
    "\n",
    "# Stack the tensors along a new dimension\n",
    "stacked_tensor = torch.stack([tensor1, tensor2, tensor3])\n",
    "stacked_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized weights:\n",
      "Parameter containing:\n",
      "tensor([[-0.2160,  0.4554,  0.4358],\n",
      "        [ 0.0618, -0.4993, -0.4779],\n",
      "        [-0.3244,  0.2271,  0.0778]], requires_grad=True)\n",
      "Output:\n",
      "tensor([ 6.7517, -9.1543, -0.1949], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "sample = torch.tensor([10.0, 10.0, 10.0])\n",
    "linear = nn.Linear(3, 3, bias=False)\n",
    "\n",
    "# Print the initialized weights\n",
    "print(\"Initialized weights:\")\n",
    "print(linear.weight)\n",
    "\n",
    "# Compute the output\n",
    "output = linear(sample)\n",
    "print(\"Output:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2160,  0.4554,  0.4358],\n",
       "        [ 0.0618, -0.4993, -0.4779],\n",
       "        [-0.3244,  0.2271,  0.0778]], requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Function\n",
    "\n",
    "The **Softmax function** is commonly used in machine learning, particularly in classification tasks. It converts a vector of raw scores (logits) into probabilities, making it easier to interpret the model's outputs as probabilities for each class.\n",
    "\n",
    "## Mathematical Definition\n",
    "\n",
    "Given a vector \\(\\mathbf{z} = [z_1, z_2, \\ldots, z_n]\\), the softmax function transforms this vector into a probability distribution over \\(n\\) classes as follows:\n",
    "\n",
    "We basically exponentiate each value in the tensor, add them up, and then individual exponentiated value divided by the total.\n",
    "\n",
    "\n",
    "## Properties of Softmax\n",
    "\n",
    "1. **Output Range**: The output of the softmax function is in the range \\((0, 1)\\), making it interpretable as probabilities.\n",
    "2. **Sum to One**: The sum of all the probabilities output by softmax is equal to 1:\n",
    "   \\[\n",
    "   \\sum_{i=1}^{n} \\sigma(\\mathbf{z})_i = 1\n",
    "   \\]\n",
    "3. **Sensitive to Input Values**: The softmax function is sensitive to the input values. Even small changes in the input can significantly change the output probabilities.\n",
    "\n",
    "## Example Usage in PyTorch\n",
    "\n",
    "Here’s an example of how to use the softmax function in PyTorch:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Create a tensor\n",
    "tensor1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# Apply softmax using torch.nn.functional.softmax()\n",
    "softmax_output = F.softmax(tensor1, dim=0)\n",
    "\n",
    "print(softmax_output) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Create a tensor\n",
    "tensor1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# Apply softmax using torch.nn.functional.softmax()\n",
    "softmax_output = F.softmax(tensor1, dim=0)\n",
    "\n",
    "print(softmax_output) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Embedding Layer in PyTorch\n",
    "\n",
    "1. **What is an Embedding Layer?**\n",
    "    - An embedding layer is a way to convert discrete items (like words or tokens) into numerical vectors (lists of numbers). These vectors are useful because they allow machine learning models to understand and process the items more effectively.\n",
    "\n",
    "2. **Setting Up the Embedding Layer**:\n",
    "    ```python\n",
    "    vocab_size = 80\n",
    "    embedding_dim = 6\n",
    "    embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "    ```\n",
    "    - **`vocab_size = 80`**: This means you have 80 unique items (like words) that you want to represent. Think of this as having a vocabulary of 80 words.\n",
    "    - **`embedding_dim = 6`**: Each of these items will be represented as a vector with 6 numbers. So each word will be converted to a list of 6 numbers.\n",
    "    - **`embedding = nn.Embedding(vocab_size, embedding_dim)`**: This line creates the actual embedding layer, which will learn to represent these 80 words in 6-dimensional space.\n",
    "\n",
    "3. **Creating Input Indices**:\n",
    "    ```python\n",
    "    input_indices = torch.LongTensor([1, 5, 3, 2])\n",
    "    ```\n",
    "    - This creates a tensor (a kind of array) that contains indices (or positions) of words you want to convert. Here, `1`, `5`, `3`, and `2` are the indices of the words you want to look up in the embedding layer. \n",
    "\n",
    "4. **Using the Embedding Layer**:\n",
    "    ```python\n",
    "    embedded_output = embedding(input_indices)\n",
    "    ```\n",
    "    - This line takes the indices you've created and looks up their corresponding embedding vectors in the layer. Each index will be replaced with its associated vector of 6 numbers.\n",
    "\n",
    "5. **Output Shape and Content**:\n",
    "    ```python\n",
    "    print(embedded_output.shape)\n",
    "    print(embedded_output)\n",
    "    ```\n",
    "    - **`embedded_output.shape`**: This will show the size of the output. Since you provided 4 indices, the output shape will be `(4, 6)`, meaning you will get 4 vectors, each with 6 numbers.\n",
    "    - **`embedded_output`**: This will display the actual vectors for the input indices. For example, if the embedding layer learned specific vectors for those indices, the output might look something like this:\n",
    "      ```\n",
    "      tensor([[0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "              [0.2, 0.3, 0.4, 0.5, 0.6, 0.7],\n",
    "              [0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n",
    "              [0.4, 0.5, 0.6, 0.7, 0.8, 0.9]])\n",
    "      ```\n",
    "    - Here, each row corresponds to one of the input indices, and each column contains one of the 6 values that represent the corresponding word.\n",
    "\n",
    "**Summary**\n",
    "\n",
    "- **Embeddings**: They convert items (like words) into numerical representations (vectors) that are easier for models to process.\n",
    "- **Look-up**: You use indices to look up the corresponding vectors in the embedding layer.\n",
    "- **Output**: The output is a collection of vectors that represent the words or tokens based on the input indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6])\n",
      "tensor([[-0.1182, -0.0551,  2.0661, -0.1224,  0.8691, -0.2577],\n",
      "        [-1.9364,  0.1501,  0.0839, -1.7630,  0.7601,  0.5639],\n",
      "        [ 0.3985,  0.8211, -1.7631, -0.2576, -0.7638,  0.1854],\n",
      "        [ 1.1235, -0.2815, -0.2051,  0.3126, -0.2904,  2.0155]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Initialize an embedding layer\n",
    "vocab_size = 80\n",
    "embedding_dim = 6\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Create some input indices\n",
    "input_indices = torch.LongTensor([1, 5, 3, 2])\n",
    "\n",
    "# Apply the embedding layer\n",
    "embedded_output = embedding(input_indices)\n",
    "\n",
    "# The output will be a tensor of shape (4, 100), where 4 is the number of inputs\n",
    "# and 100 is the dimensionality of the embedding vectors\n",
    "print(embedded_output.shape)\n",
    "print(embedded_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Multiplication of 3x2 and 2x3 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 27,  30,  33],\n",
      "        [ 61,  68,  75],\n",
      "        [ 95, 106, 117]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,2],[3,4],[5,6]])\n",
    "b = torch.tensor([[7,8,9],[10,11,12]])\n",
    "# print(a @ b)\n",
    "print(torch.matmul(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[0.5358, 0.3800, 0.2044],\n",
      "        [0.2654, 0.4078, 0.5171]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "int_64 = torch.randint(1, (3, 2)).float()  \n",
    "print(int_64)\n",
    "# 1 is the lower bound, (3, 2) is the shape of the tensor, which will have 3 rows and 2 columns.\n",
    "# type int64\n",
    "float_32 = torch.rand(2,3) #This converts the integer tensor to a float tensor\n",
    "# print(int_64.dtype, float_32.dtype)\n",
    "print(float_32)\n",
    "result = torch.matmul(int_64, float_32)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Tensors and Shape\n",
    "Check the following code\n",
    "- The first dimension (2) represents the number of \"batches\" or 2 \"slices.\"\n",
    "- The second dimension (3) represents the number of rows in each slice.\n",
    "- The third dimension (5) represents the number of elements in each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5])\n",
      "torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 3, 5)\n",
    "print(a.shape)\n",
    "x, y, z = a.shape\n",
    "a = a.view(x,y,z)\n",
    "# print(x, y, z)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.4784e-01, 7.1257e-01, 5.8318e-02, 1.1662e-01, 5.6686e-01, 1.9303e-01,\n",
      "         6.6632e-01, 6.3045e-01, 1.1654e-01, 2.2591e-01],\n",
      "        [4.4540e-01, 1.8662e-01, 1.6590e-01, 7.8310e-01, 2.8952e-01, 3.6106e-01,\n",
      "         2.4187e-01, 5.7216e-01, 5.1018e-01, 8.8388e-01],\n",
      "        [2.1610e-01, 4.2283e-01, 2.7590e-01, 1.3142e-02, 2.8317e-02, 9.7419e-01,\n",
      "         9.3365e-01, 6.6609e-01, 5.2375e-01, 8.5033e-02],\n",
      "        [7.0206e-01, 5.1114e-01, 7.0352e-03, 2.6985e-01, 9.7451e-01, 1.1618e-01,\n",
      "         5.5879e-01, 8.5950e-01, 3.5737e-01, 6.6099e-01],\n",
      "        [8.6752e-01, 1.4063e-01, 1.3130e-01, 4.8753e-01, 1.3561e-01, 2.6779e-01,\n",
      "         4.9860e-01, 9.8521e-01, 2.7027e-01, 8.7181e-01],\n",
      "        [9.0724e-04, 9.4394e-01, 5.9852e-01, 2.3980e-01, 2.1402e-02, 2.2790e-01,\n",
      "         9.2466e-01, 1.5136e-01, 3.5337e-01, 2.2062e-01],\n",
      "        [8.0447e-01, 4.4472e-01, 5.8952e-01, 4.1328e-01, 8.5890e-01, 6.8109e-01,\n",
      "         4.5790e-01, 6.8573e-01, 7.8548e-01, 2.3513e-01],\n",
      "        [1.1809e-01, 5.6395e-01, 1.0018e-01, 4.3966e-01, 6.3728e-01, 8.3437e-01,\n",
      "         8.9655e-01, 3.1252e-01, 8.8037e-02, 1.1689e-01],\n",
      "        [9.9730e-01, 1.8197e-01, 5.5324e-01, 4.2488e-01, 2.3790e-03, 2.5658e-01,\n",
      "         4.1206e-01, 8.6530e-01, 6.9251e-01, 8.8614e-01],\n",
      "        [1.1549e-02, 9.5455e-01, 3.8725e-01, 8.5537e-01, 7.4989e-01, 8.2071e-01,\n",
      "         5.9832e-01, 3.0360e-01, 5.4594e-01, 8.4376e-01],\n",
      "        [1.3777e-01, 4.1883e-01, 5.6526e-01, 7.4036e-02, 8.3878e-01, 1.6556e-02,\n",
      "         6.9582e-01, 7.0584e-01, 4.3799e-01, 3.5505e-02],\n",
      "        [9.8989e-01, 5.1932e-01, 7.0095e-01, 7.2378e-01, 6.6765e-01, 9.8325e-01,\n",
      "         2.9214e-01, 7.4497e-01, 2.8286e-01, 9.9305e-01],\n",
      "        [8.8807e-01, 8.8728e-01, 7.7809e-01, 2.0169e-01, 8.1256e-01, 3.0491e-01,\n",
      "         6.3829e-01, 4.2424e-01, 3.8111e-01, 3.7713e-01],\n",
      "        [8.6794e-01, 2.4137e-01, 9.4477e-01, 3.7884e-01, 6.0674e-01, 2.4725e-02,\n",
      "         1.2078e-01, 5.0054e-01, 8.3786e-01, 4.7303e-01],\n",
      "        [6.4304e-01, 8.3795e-02, 8.6081e-03, 9.3872e-01, 1.3191e-01, 9.0981e-01,\n",
      "         2.6003e-01, 1.5305e-02, 3.7633e-01, 5.9801e-02],\n",
      "        [9.0000e-01, 5.5200e-01, 5.3571e-02, 8.6615e-01, 5.3864e-01, 1.9017e-01,\n",
      "         1.0007e-01, 6.0474e-01, 5.8185e-02, 1.3151e-01],\n",
      "        [6.5226e-01, 5.3695e-01, 8.5617e-01, 9.6370e-01, 5.5281e-01, 5.8211e-01,\n",
      "         6.1119e-01, 7.7745e-01, 3.3720e-01, 3.5373e-01],\n",
      "        [8.7052e-01, 2.7166e-01, 6.5231e-01, 1.0057e-01, 1.0628e-01, 6.8225e-01,\n",
      "         9.5398e-01, 5.2178e-01, 8.5180e-01, 5.2549e-01],\n",
      "        [2.2479e-01, 4.1928e-01, 8.0073e-01, 8.4228e-01, 3.8563e-01, 1.5114e-01,\n",
      "         6.8264e-01, 4.5133e-01, 4.8565e-01, 8.7312e-01],\n",
      "        [3.7959e-02, 5.0993e-01, 8.9903e-01, 6.8165e-01, 4.8601e-01, 8.6840e-01,\n",
      "         1.0306e-01, 1.1990e-02, 1.5555e-01, 6.9926e-01],\n",
      "        [6.9057e-02, 7.6252e-01, 6.3782e-01, 8.0156e-01, 7.3988e-01, 7.5775e-01,\n",
      "         4.8732e-01, 4.1785e-01, 4.5084e-01, 8.3595e-01],\n",
      "        [6.1156e-01, 8.5702e-01, 9.8226e-02, 7.8539e-02, 4.8247e-01, 2.7219e-01,\n",
      "         2.4549e-01, 9.6610e-01, 3.2557e-01, 5.5234e-01],\n",
      "        [3.4030e-01, 6.7352e-01, 7.8608e-01, 6.7252e-01, 7.0807e-01, 5.0895e-01,\n",
      "         4.2008e-01, 4.1877e-01, 3.6420e-01, 3.8804e-01],\n",
      "        [7.5602e-01, 8.6862e-01, 1.7415e-01, 9.5588e-01, 8.8406e-01, 1.9198e-01,\n",
      "         4.5605e-01, 7.4624e-01, 9.9847e-02, 7.9370e-01],\n",
      "        [4.2986e-01, 3.7908e-01, 9.7233e-01, 5.2798e-01, 7.7853e-01, 6.7777e-01,\n",
      "         5.0770e-01, 8.4769e-01, 6.7054e-01, 2.4795e-01],\n",
      "        [3.4168e-01, 1.1087e-01, 9.6107e-01, 4.6941e-01, 6.6239e-01, 1.4574e-01,\n",
      "         6.6885e-01, 9.4786e-01, 8.0510e-02, 1.1269e-01],\n",
      "        [3.4333e-02, 9.2557e-01, 5.1897e-01, 9.7018e-01, 7.0402e-01, 9.8951e-01,\n",
      "         1.1039e-01, 4.1054e-01, 9.5528e-01, 7.6245e-01],\n",
      "        [9.5671e-01, 1.8762e-02, 3.0689e-01, 4.4409e-01, 2.1237e-01, 7.5092e-02,\n",
      "         5.7327e-01, 3.9093e-01, 5.5734e-01, 6.3038e-01],\n",
      "        [6.8145e-01, 1.6966e-02, 4.5854e-01, 5.3712e-01, 3.0549e-01, 5.4741e-02,\n",
      "         9.9189e-01, 6.1542e-01, 5.1386e-01, 3.4015e-01],\n",
      "        [1.9131e-01, 5.8434e-01, 7.5250e-01, 6.0344e-01, 1.4139e-01, 5.0211e-01,\n",
      "         9.6055e-01, 3.6290e-01, 6.8361e-01, 5.4033e-01],\n",
      "        [8.7327e-01, 1.2722e-01, 5.5897e-01, 5.0354e-01, 4.5040e-01, 6.8135e-01,\n",
      "         8.6287e-01, 6.9475e-01, 7.9137e-01, 3.5860e-01],\n",
      "        [7.9182e-01, 1.8288e-01, 2.6567e-01, 3.1575e-01, 8.3716e-01, 3.6981e-01,\n",
      "         7.7873e-03, 8.2700e-01, 5.3342e-01, 3.9319e-01]])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# print(input)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "input = torch.rand((4, 8, 10))\n",
    "B, T, C = input.shape\n",
    "# B: a batch size or number of sequences\n",
    "# T: the sequence length or the number of time steps (T)\n",
    "# C: the number of features or channels (C) at each time step.\n",
    "output = input.view(B*T, C) #Making it a 2D vector\n",
    "print(output)\n",
    "# print(input)\n",
    "print(output[:, -1, :]) #Will throw an error as we only have a 2d view now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([10], dtype=torch.float32)\n",
    "y = F.tanh(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu kernel",
   "language": "python",
   "name": "gpu_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
